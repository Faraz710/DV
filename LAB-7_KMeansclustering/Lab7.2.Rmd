---
title: "CSE3020 Lab7"
author: "Faraz Suhail"
date: "15/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Name: "Faraz Suhail"
## Reg. No.: "19BCE1525"


```{r}
library(ggplot2)
rm(list=ls())
#Load and view dataset
require("datasets")
data("iris") # load Iris Dataset
str(iris) #view structure of dataset
```

```{r}
#2. Display the Statistical Summary of the dataset
summary(iris) #view statistical summary of dataset
head(iris) #view top rows of dataset
```

```{r}
#3. Apply the preprocessing to remove the class attribute eg., Species. Since clustering is a type of Unsupervised Learning, we would not require Class Label(output) during execution of our algorithm. We will, therefore, remove Class Attribute and store it in another variable.
iris.new<- iris[,c(1,2,3,4)]
iris.class<- iris[,"Species"]
head(iris.new)
head(iris.class)
```

```{r}
#4. Create a function to normalize the data before clustering
normalize <- function(x){
return ((x-min(x))/(max(x)-min(x)))
}
iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
```


```{r}
#5. Apply k-means clustering algorithm with k = 3
result<- kmeans(iris.new,3) #apply k-means algorithm with no. of centroids(k)=3
#6. Find the number of records in each cluster
result$size # gives no. of records in each cluster
#7. Display the cluster center data point values
result$centers # gives value of cluster center datapoint value(3 centers for k=3)
#8. Display the cluster vector showing the cluster where each record falls
result$cluster #gives cluster vector showing the cluster where each record falls
```

```{r}
# Verify results of clustering
par(mfrow=c(2,2), mar=c(5,4,2,2))

#9. Plot to see how Sepal.Length and Sepal.Width data points have been distributed in clusters
plot(iris.new[c(1,2)], col=result$cluster)

#10. Plot to see how Sepal.Length and Sepal.Width data points have been distributed originally as per "class" attribute in dataset
plot(iris.new[c(1,2)], col=iris.class)

#11.Plot to see how Petal.Length and Petal.Width data points have been distributed in clusters
plot(iris.new[c(3,4)], col=result$cluster)

#12.Plot to see how Petal.Length and Petal.Width data points have been distributed originally as per "class" attribute in dataset
plot(iris.new[c(3,4)], col=iris.class)
result$cluster <- as.factor(result$cluster)
```

```{r}
#14. Plot the clusterresults using ggplot
ggplot(iris.new, aes(Petal.Length, Petal.Width, color = result$cluster)) + geom_point()
plot(iris.new[c("Sepal.Length", "Sepal.Width")], col=result$cluster)
```


```{r}
#16. Display the results in table
table(result$cluster,iris.class)
#Total number of correctly classified instances are: 36 + 47 + 50= 133
#Total number of incorrectly classified instances are: 3 + 14= 17
#Accuracy = 133/(133+17) = 0.88 i.e our model has achieved 88% accuracy!
```

```{r}
#17. Display the K Means Algorithm with Animation and visualize the changes in the cluster center
library(animation)
km1<-kmeans.ani(iris.new,3)

#18. Import factoextra package and visualize the cluster result
library(factoextra) # clustering algorithms & visualization
fviz_cluster(result, data = iris.new)
#19. Explore the cluster analysis result with various value of k like 3,4,5
k2 <- kmeans(iris.new, centers = 2, nstart = 25)
k3 <- kmeans(iris.new, centers = 3, nstart = 25)
k4 <- kmeans(iris.new, centers = 4, nstart = 25)
k5 <- kmeans(iris.new, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = iris.new) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = iris.new) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = iris.new) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = iris.new) + ggtitle("k = 5")
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```